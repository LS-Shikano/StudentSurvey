# Batch training
for (i in 1:num_batches) {
start_idx <- (i - 1) * batch_size + 1
end_idx <- min(i * batch_size, nrow(X_train_fold))
X_batch <- torch_tensor(as.matrix(X_train_shuffled[start_idx:end_idx, ]), dtype = torch_float())
y_batch <- torch_tensor(as.numeric(y_train_shuffled[start_idx:end_idx]), dtype = torch_float())$view(c(-1, 1))
optimizer$zero_grad()
outputs <- model(X_batch)
loss <- criterion(outputs, y_batch)
loss$backward()
optimizer$step()
total_loss <- total_loss + loss$item()
}
# Model evaluation and validation
avg_train_loss <- total_loss / num_batches
train_loss_history <- c(train_loss_history, avg_train_loss)
model$eval()
val_outputs <- model(torch_tensor(as.matrix(X_val_fold), dtype = torch_float()))
val_loss <- criterion(val_outputs, torch_tensor(as.numeric(y_val_fold), dtype = torch_float())$view(c(-1, 1)))
val_loss_history <- c(val_loss_history, val_loss$item())
train_accuracy <- mean(as.factor(round(as.numeric(model(torch_tensor(as.matrix(X_train_fold), dtype = torch_float()))))) == as.factor(as.numeric(y_train_fold)))
train_accuracy_history <- c(train_accuracy_history, train_accuracy)
test_outputs <- model(torch_tensor(as.matrix(X_val_fold), dtype = torch_float()))
test_accuracy <- mean(as.factor(round(as.numeric(test_outputs))) == as.factor(as.numeric(y_val_fold)))
test_accuracy_history <- c(test_accuracy_history, test_accuracy)
# Early stopping
if (val_loss$item() < best_val_loss) {
best_val_loss <- val_loss$item()
epochs_without_improvement <- 0
best_model_state <- model$state_dict()
} else {
epochs_without_improvement <- epochs_without_improvement + 1
}
if (epochs_without_improvement >= patience) {
break  # Stop training if no improvement
}
}
if (!is.null(best_model_state)) {
model$load_state_dict(best_model_state)
}
best_model_history <- c(best_model_history, test_accuracy)
print(test_accuracy)
}
print(mean(best_model_history))
#| message: false
#| warning: false
history <- data.frame(
epoch = 1:length(train_loss_history),
train_loss = train_loss_history,
val_loss = val_loss_history,
train_accuracy = train_accuracy_history,
test_accuracy = test_accuracy_history
)
df_melt <- melt(history, id.vars = "epoch",
variable.name = "metric", value.name = "value")
ggplot(df_melt, aes(x = epoch, y = value, color = metric)) +
geom_line(size = .5, alpha=0.8) +
labs(y = "Accuracy and Loss", x = "Training Epochs counted over all Folds") +
scale_color_manual(values = c("train_loss" = "blue", "val_loss" = "red",
"train_accuracy" = "green", "test_accuracy" = "purple"),
labels = c("Train Loss", "Validation Loss",
"Train Accuracy", "Validation Accuracy")) +
theme_dark()
k <- 5  # Number of folds
folds <- sample(rep(1:k, length.out = nrow(X)))
best_model_history_rf <- numeric()
# K-Fold Cross-Validation
for (fold in 1:k) {
print(paste("Testing on fold", fold, "and training on all other folds"))
train_indices <- which(folds != fold)
val_indices <- which(folds == fold)
X_train_fold <- X[train_indices, ]
y_train_fold <- y[train_indices]
X_val_fold <- X[val_indices, ]
y_val_fold <- y[val_indices]
# Train Random Forest model
rf_model <- randomForest(
x = X_train_fold,               # Training data (features)
y = as.factor(y_train_fold),     # Target variable (as factor for classification)
ntree = 500,                     # Number of trees (adjustable)
mtry = sqrt(ncol(X_train_fold)), # Features considered at each split
importance = TRUE,               # Calculate feature importance
proximity = TRUE
)
predictions <- predict(rf_model, newdata = X_val_fold)
test_accuracy <- mean(predictions == as.factor(y_val_fold))
best_model_history_rf <- c(best_model_history_rf, test_accuracy)
}
# Print the accuracy for each fold
print(mean(best_model_history_rf))
#confusion matrix
confusion_rf <- confusionMatrix((predict(rf_model, X_val_fold)), as.factor(y_val_fold))
confusion_rf_df <- as.data.frame(confusion_rf$table)
confusion_rf_df$Data <- "Random Forest"
confusion_train <- confusionMatrix(as.factor(round(as.numeric(model(X_val_fold)))), as.factor(y_val_fold))
confusion_train_df <- as.data.frame(confusion_train$table)
confusion_train_df$Data <- "Neural Network"
confusion_df <- rbind(confusion_train_df, confusion_rf_df)
# Create a heatmap for the confusion matrices
ggplot(confusion_df, aes(x = Reference, y = Prediction, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), color = "white", size = 5) +
facet_wrap(~ Data) +
labs(x = "Reference",
y = "Prediction",
fill = "Frequency") +
theme_dark() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
#metrics histplot
NeuralNetwork <- confusion_train$byClass
RandomForest <- confusion_rf$byClass
combined_metrics_df <- cbind(
NN = as.data.frame(NeuralNetwork),
RF = as.data.frame(RandomForest)
)
combined_metrics_df <- combined_metrics_df %>%
rownames_to_column(var = "Metric")
long_metrics_df <- combined_metrics_df %>%
pivot_longer(
cols = -Metric,
names_to = "Data",
values_to = "Value"
)
ggplot(long_metrics_df, aes(x = Metric, y = Value, fill = Data)) +
geom_bar(stat = "identity", position = "dodge") +
labs(
x = "Metric",
y = "Value",
fill = "Data"
) +
theme_dark() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
#accuracy with CI
ci_result <- t.test(best_model_history, conf.level = 0.95)
ci_result_rf <- t.test(best_model_history_rf, conf.level = 0.95)
NeuralNetwork <- c(ci_result$conf.int[1], ci_result$conf.int[2])
RandomForest <- c(ci_result_rf$conf.int[1], ci_result_rf$conf.int[2])
combined_metrics_df <- data.frame(
Model = c("Neural Network", "Random Forest"),
AccuracyLower = c(NeuralNetwork[1], RandomForest[1]),
AccuracyUpper = c(NeuralNetwork[2], RandomForest[2])
)
combined_metrics_df$MeanAccuracy <- rowMeans(combined_metrics_df[, c("AccuracyLower", "AccuracyUpper")])
# Plot the confidence intervals
ggplot(combined_metrics_df, aes(x = Model, y = MeanAccuracy)) +
geom_point(size = 4, color = "red") +  # Mean accuracy
geom_errorbar(aes(ymin = AccuracyLower, ymax = AccuracyUpper), width = 0.2, color = "blue") +  # Confidence intervals
geom_text(aes(label = paste("Mean:", round(MeanAccuracy, 3),
"\nCI: [", round(AccuracyLower, 3), ", ", round(AccuracyUpper, 2), "]")),
vjust = -1, hjust=1, color = "black", size = 4) +  # Add mean accuracy and CI labels
labs(
x = "Model",
y = "Accuracy") +
theme_dark()
library(PRROC)
#Precision-recall Plot
true_labels_numeric <- y_val_fold
nn_probabilities <- as.numeric(model(X_val_fold))
pr_nn <- pr.curve(scores.class0 = nn_probabilities,
weights.class0 = true_labels_numeric,
curve = TRUE)
rf_probabilities <- predict(rf_model, X_val_fold, type = "prob")[, 2]
pr_rf <- pr.curve(scores.class0 = rf_probabilities,
weights.class0 = true_labels_numeric,
curve = TRUE)
pr_curves <- bind_rows(
data.frame(pr_nn$curve, Model = "Neural Network"),
data.frame(pr_rf$curve, Model = "Random Forest")
)
colnames(pr_curves) <- c("Recall", "Precision", "Threshold", "Model")
auc_values <- c(
"Random Forest" = pr_rf$auc.integral,
"Neural Network" = pr_nn$auc.integral
)
ggplot(pr_curves, aes(x = Recall, y = Precision, color = Model)) +
geom_line(linewidth = 1) +
geom_hline(yintercept = mean(true_labels_numeric),
linetype = "dashed", color = "gray") +
coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
labs(title = sprintf("AUC RF = %.3f, AUC NN = %.3f",
auc_values["Random Forest"], auc_values["Neural Network"]),
x = "Recall",
y = "Precision") +
theme_dark() +
theme(plot.title = element_text(hjust = 0.5))
df_pca <- prcomp(X, center = TRUE, scale. = TRUE)
pca_df <- data.frame(df_pca$x[, 1:2], Target = as.factor(df$Quality))
colnames(pca_df) <- c("PC1", "PC2", "Target")
# Scree Plot: Variance explained by each principal component
explained_var <- df_pca$sdev^2 / sum(df_pca$sdev^2) * 100
scree_df <- data.frame(PC = seq_along(explained_var), Variance = explained_var)
# PCA Loadings: Most important variables for PC1 and PC2
loadings <- as.data.frame(df_pca$rotation[, 1:2])
loadings$Feature <- rownames(loadings)
top_features_pc1 <- loadings %>% arrange(desc(abs(PC1))) %>% head(5)
top_features_pc2 <- loadings %>% arrange(desc(abs(PC2))) %>% head(5)
ggplot(scree_df, aes(x = PC, y = Variance)) +
geom_bar(stat = "identity", fill = "steelblue", alpha = 0.7) +
geom_line(aes(group = 1), color = "black") +
geom_point(color = "red", size = 2) +
labs(x = "Principal Component", y = "Explained Variance (%)") +
theme_dark()
ggplot(pca_df, aes(x = PC1, y = PC2, color = Target)) +
geom_point(alpha = 0.5, size = 1) +
labs(x = "Principal Component 1", y = "Principal Component 2") +
theme_dark() +
scale_color_discrete(name = "Quality")
ggplot(loadings, aes(x = PC1, y = PC2, label = Feature)) +
geom_vline(xintercept = 0, linetype = "dashed", color = "gray") +
geom_hline(yintercept = 0, linetype = "dashed", color = "gray") +
geom_point(color = "blue", alpha = 0.6) +
geom_text_repel(size = 4, color = "black") +
labs(x = "PC1 Contribution", y = "PC2 Contribution") +
theme_dark()
#| message: false
#| warning: false
# Load and preprocess data
df <- read_csv("/Users/ramius/Desktop/CodeVault/01_Project/Uni/SL_StatisticalLearning/SL Exam/apple_quality.csv")
df <- as.data.frame(df)
df <- df[1:4000, ] # exclude last NA value
df$Quality <- as.integer(df$Quality == "good")  # 1 is good, 0 is bad
df <- df[, 2:9]  # Drop unique ID's
# Split into features (X) and target (y)
X <- as.matrix(df[, -ncol(df)])  # All columns except the last because target
X <- apply(X, 2, as.numeric)     # Ensure all columns are numeric
X <- scale(X,center = TRUE, scale = TRUE)                    # Standardize features
y <- df[, ncol(df)]              # Last column is the target
# Create a histogram
ggplot(df, aes(x = Quality)) +
geom_bar(fill = "purple", color = "black") +
labs(title = "Quality Distribution for Dataset", x = "Quality Category", y = "Count") +
theme_dark()
library(reticulate)
#use_python("/Users/ramius/.pyenv/versions/3.12.0/bin", required = TRUE)
library(devtools)
library(networkdata)
library(jsonlite)
library(networkdata)
# Load libraries
library(igraph)
library(statnet)
setwd("/Users/ramius/Desktop/CodeVault/01_Project/Work/Susumu/Student_Survey/StudentSurvey_GitDock/data/NA")
setwd("~/Desktop/CodeVault/01_Project/Work/Susumu/Student_Survey/StudentSurvey_GitDock/data_WS2425/NA")
# Read graph from GML file using igraph
G_igraph <- read.graph("multiplex_pgrah_w3.gml", format = "gml")
# Read graph from GML file using igraph
G_igraph <- read.graph("multiplex_graph_w3.gml", format = "gml")
summary(G_igraph)
# Read graph from GML file using igraph
G_multi <- read.graph("multiplex_graph_w3.gml", format = "gml")
G_attri <- read.graph("graph_edge_attributes_w3.gml", format = "gml")
summary(G_igraph)
# Read graph from GML file using igraph
G_multi <- read.graph("multiplex_graph_w3.gml", format = "gml")
G_attri <- read.graph("graph_edge_attributes_w3.gml", format = "gml")
summary(G_multi)
summary(G_attri)
# Convert igraph graph to adjacency matrix
adj_matrix <- as.matrix(as_adjacency_matrix(G_igraph))
# Convert the adjacency matrix to a statnet network object
G_statnet <- as.network(adj_matrix, directed = is_directed(G_igraph))
# Step 3: Extract node attributes from igraph
node_attributes <- vertex_attr(G_igraph)
for (attr_name in names(node_attributes)) {
set.vertex.attribute(G_statnet, attr_name, node_attributes[[attr_name]])
}
# Step 3: Extract node attributes from igraph
node_attributes <- vertex_attr(G_igraph)
for (attr_name in names(node_attributes)) {
set_vertex_attr(G_statnet, attr_name, node_attributes[[attr_name]])
}
# Convert igraph graph to adjacency matrix
adj_matrix <- as.matrix(as_adjacency_matrix(G_multi))
# Convert the adjacency matrix to a statnet network object
G_statnet <- as.network(adj_matrix, directed = is_directed(G_igraph))
# Convert igraph graph to adjacency matrix
adj_matrix <- as.matrix(as_adjacency_matrix(G_multi))
# Convert the adjacency matrix to a statnet network object
G_statnet <- as.network(adj_matrix, directed = is_directed(G_multi))
# Step 3: Extract node attributes from igraph
node_attributes <- vertex_attr(G_multi)
for (attr_name in names(node_attributes)) {
set_vertex_attr(G_statnet, attr_name, node_attributes[[attr_name]])
}
# Step 3: Extract node attributes from igraph
node_attributes <- vertex_attr(G_multi)
for (attr_name in names(node_attributes)) {
set_vertex_attr(G_statnet, attr_name, node_attributes[[attr_name]])
}
G_multi.edges()
G_multi.get_edges()
install.packages("RSiena")
library(reticulate)
#use_python("/Users/ramius/.pyenv/versions/3.12.0/bin", required = TRUE)
library(devtools)
library(networkdata)
library(jsonlite)
library(networkdata)
# Load libraries
library(igraph)
library(statnet)
library(RSiena)
setwd("~/Desktop/CodeVault/01_Project/Work/Susumu/Student_Survey/StudentSurvey_GitDock/data_WS2425/NA")
# Read graph from GML file using igraph
G_multi <- read.graph("multiplex_graph_w3.gml", format = "gml")
G_attri <- read.graph("graph_edge_attributes_w3.gml", format = "gml")
summary(G_multi)
summary(G_attri)
View(G_attri)
G_multi
G_attri
E(G_attri)
E(G_attri)$weight
E(G_attri)$weights
E(G_multi)$weights
E(G_multi)$weight
# Read graph from GML file using igraph
G_multi <- read.graph("multiplex_graph_w3.gml", format = "gml")
summary(G_multi)
E(G_multi)$weight
E(G_multi)
E(G_multi)$type
# Read graph from GML file using igraph
G_multi_w3 <- read.graph("multiplex_graph_w3.gml", format = "gml")
summary(G_multi)
E(G_multi_w3)
# Convert igraph graph to adjacency matrix
adj_matrix <- as.matrix(as_adjacency_matrix(G_multi_w3))
# Convert the adjacency matrix to a statnet network object
G_statnet <- as.network(adj_matrix, directed = is_directed(G_multi_w3))
# Step 3: Extract node attributes from igraph
node_attributes <- vertex_attr(G_multi_w3)
for (attr_name in names(node_attributes)) {
set_vertex_attr(G_statnet, attr_name, node_attributes[[attr_name]])
}
# Convert igraph graph to adjacency matrix
adj_matrix <- as.matrix(as_adjacency_matrix(G_multi_w3))
# Convert the adjacency matrix to a statnet network object
G_statnet <- as.network(adj_matrix, directed = is_directed(G_multi_w3))
# Step 3: Check the summary of the network object
summary(G_statnet)
class(G_statnet)
surveymodel.01 <- ergm(G_statnet~edges)
summary(surveymodel.01)
bottmodel.03 <- ergm(G_statnet~edges+nodecov('age'))
G_statnet
N(G_statnet)
V(G_statnet)
V(G_multi)
E(G_multi_w3)$friend
# Read graph from GML file using igraph
G_multi_w3 <- read.graph("multiplex_graph_w3.gml", format = "gml")
summary(G_multi)
E(G_multi_w3)$type
surveymodel.01 <- ergm(G_multi_w3~edges)
surveymodel.01 <- ergm(G_statnet~edges)
summary(surveymodel.01)
# Step 3: Extract node attributes from igraph
node_attributes <- vertex_attr(G_multi_w3)
for (attr_name in names(node_attributes)) {
set.vertex.attribute(G_statnet, attr_name, node_attributes[[attr_name]])
}
node_attributes
# Step 3: Extract node attributes from igraph
node_attributes <- vertex_attr(G_multi_w3)
for (attr_name in names(node_attributes)) {
set.vertex.attribute(G_statnet, attr_name, node_attributes[[attr_name]])
}
# Step 3: Extract node attributes from igraph
node_attributes <- vertex_attr(G_multi_w3)
for (attr_name in names(node_attributes)) {
set.vertex.attribute(G_statnet, attr_name, node_attributes[[attr_name]])
}
for (attr_name in names(node_attributes)) {
set.vertex.attribute(G_statnet, attr_name, node_attributes[[attr_name]])
for (attr_name in names(node_attributes)) {
set.vertex.attribute(G_statnet, attr_name, node_attributes[[attr_name]])
}
class(G_statnet)
for (attr_name in names(node_attributes)) {
set.vertex.attribute(G_statnet, attr_name, node_attributes[[attr_name]])
}
node_attributes
# Assuming G_statnet is your network object and node_attributes is a list of attributes
for (attr_name in names(node_attributes)) {
set.vertex.attribute(G_statnet, attrname = attr_name, value = node_attributes[[attr_name]])
}
# Assuming G_statnet is your network object and node_attributes is a list of attributes
for (attr_name in names(node_attributes)) {
set.vertex.attribute(G_statnet, value = node_attributes[[attr_name]])
}
class(node_attributes)
# Define node attributes
node_attributes <- list(
gender = c("Male", "Female", "Male", "Female", "Male"),
age = c(25, 30, 22, 28, 35)
)
# Set the vertex attributes
for (attr_name in names(node_attributes)) {
set.vertex.attribute(G_statnet, attrname = attr_name, value = node_attributes[[attr_name]])
}
V(G_multi_w3)$age
# Step 3: Set the vertex attributes
for (attr_name in names(node_attributes)) {
set.vertex.attribute(G_statnet, attr_name, node_attributes[[attr_name]])
}
E(G_multi_w3)[type == friend]
E(G_multi_w3)[type == "friend"]
# Step 1: Convert igraph graph to adjacency matrix
adj_matrix <- as.matrix(as_adjacency_matrix(G_multi_w3))
# Step 2: Convert the adjacency matrix to a statnet network object
G_statnet <- as.network(adj_matrix, directed = is_directed(G_multi_w3))
# Step 3: Extract node attributes from igraph
node_attributes <- vertex_attr(G_multi_w3)
# Step 4: Set vertex attributes in the statnet network object
for (attr_name in names(node_attributes)) {
set.vertex.attribute(G_statnet, attrname = attr_name, value = node_attributes[[attr_name]])
}
#library(reticulate)
#use_python("/Users/ramius/.pyenv/versions/3.12.0/bin", required = TRUE)
library(devtools)
library(networkdata)
library(jsonlite)
library(networkdata)
# Load libraries
library(igraph)
library(statnet)
#library(RSiena)
# Step 1: Convert igraph graph to adjacency matrix
adj_matrix <- as.matrix(as_adjacency_matrix(G_multi_w3))
# Step 2: Convert the adjacency matrix to a statnet network object
G_statnet <- as.network(adj_matrix, directed = is_directed(G_multi_w3))
# Step 3: Extract node attributes from igraph
node_attributes <- vertex_attr(G_multi_w3)
# Step 4: Set vertex attributes in the statnet network object
for (attr_name in names(node_attributes)) {
set.vertex.attribute(G_statnet, attrname = attr_name, value = node_attributes[[attr_name]])
}
# Step 1: Convert igraph graph to adjacency matrix
adj_matrix <- as.matrix(as_adjacency_matrix(G_multi_w3))
# Step 2: Convert the adjacency matrix to a statnet network object
G_statnet <- as.network(adj_matrix, directed = is_directed(G_multi_w3))
# Step 3: Extract node attributes from igraph
node_attributes <- vertex_attr(G_multi_w3)
# Step 4: Set vertex attributes in the statnet network object
for (attr_name in names(node_attributes)) {
set.vertex.attribute(G_statnet, value = node_attributes[[attr_name]])
}
surveymodel.01 <- ergm(G_statnet~edges)
summary(surveymodel.01)
surveymodel.01 <- ergm(G_multi_w3~edges)
# Step 2: Convert the adjacency matrix to a statnet network object
G_statnet <- as.network(G_multi_w3, directed = is_directed(G_multi_w3))
# Step 2: Convert the adjacency matrix to a statnet network object
G_statnet <- as.network(E(G_multi_w3)[type == "friend"], directed = is_directed(G_multi_w3))
(G_multi_w3)[type == "friend"]
G_multi_w3[type == "friend"]
E(G_multi_w3)[type == "friend"]
friend_edges <- E(G_multi_w3)[type == "friend"]
G_friend <- induced_subgraph(G_multi_w3, which(E(G_multi_w3) %in% friend_edges))
friend_edges <- E(G_multi_w3)[E(G_multi_w3)$type == "friend"]
vertices_involved <- unique(c(ends(G_multi_w3, friend_edges)))
G_friend <- induced_subgraph(G_multi_w3, vertices = vertices_involved)
# Step 1: Filter edges by type "friend"
friend_edges <- E(G_multi_w3)[E(G_multi_w3)$type == "friend"]
# Step 2: Identify vertices involved in "friend" edges
vertices_involved <- unique(c(ends(G_multi_w3, friend_edges)))
# Step 3: Create subgraph
G_friend <- induced_subgraph(G_multi_w3, vertices = vertices_involved)
# Step 1: Filter edges by type "friend"
friend_edges <- E(G_multi_w3)[E(G_multi_w3)$type == "friend"]
# Step 2: Identify vertices involved in "friend" edges
vertices_involved <- unique(c(ends(G_multi_w3, friend_edges)))
# Step 3: Create subgraph
G_friend <- induced_subgraph(G_multi_w3)
friend_edges <- E(G_multi_w3)[E(G_multi_w3)$type == "friend"]
G_friend <- induced_subgraph(G_multi_w3, vids = friend_edges, impl = "auto")
friend_edges <- E(G_multi_w3)[E(G_multi_w3)$type == "friend"]
vertices_involved <- unique(c(ends(G_multi_w3, friend_edges)))
G_friend <- induced_subgraph(G_multi_w3, vids = vertices_involved, impl = "auto")
friend_edges <- E(G_multi_w3)[E(G_multi_w3)$type == "friend"]
vertices_involved <- unique(c(ends(G_multi_w3, friend_edges)))
G_friend <- induced_subgraph(G_multi_w3, vids = vertices_involved, impl = "auto")
# Step 2: Convert the adjacency matrix to a statnet network object
G_statnet <- as.network(G_friend, directed = is.directed(G_multi_w3))
friend_edges <- E(G_multi_w3)[E(G_multi_w3)$type == "friend"]
vertices_involved <- unique(c(ends(G_multi_w3, friend_edges)))
G_friend <- induced_subgraph(G_multi_w3, vids = vertices_involved, impl = "auto")
# Step 2: Convert the adjacency matrix to a statnet network object
G_statnet <- as.network(G_friend, directed = is.directed(G_multi_w3))
install.packages("intergraph")
#library(reticulate)
#use_python("/Users/ramius/.pyenv/versions/3.12.0/bin", required = TRUE)
library(devtools)
library(networkdata)
library(jsonlite)
library(networkdata)
# Load libraries
library(igraph)
library(statnet)
library(intergraph)
#library(RSiena)
# Read graph from GML file using igraph
G_multi_w3 <- read.graph("multiplex_graph_w3.gml", format = "gml")
summary(G_multi)
setwd("~/Desktop/CodeVault/01_Project/Work/Susumu/Student_Survey/StudentSurvey_GitDock/data_WS2425/NA")
# Read graph from GML file using igraph
G_multi_w3 <- read.graph("multiplex_graph_w3.gml", format = "gml")
G_multi_w3
class(G_multi_w3)
G_statnet_w3 <- asNetwork(G_multi_w3)
G_statnet_w3
