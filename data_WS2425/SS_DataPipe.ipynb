{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_85829/1169033480.py:8: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_w2[\"participant.label\"][244] = \"nan\" # fixing the missing label\n",
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_85829/1169033480.py:12: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_w3[\"participant.label\"][46] = \"nan\" # fixing the missing label\n"
     ]
    }
   ],
   "source": [
    "df_w1 = pd.read_csv(\"Raw/W1_AllAppsWide_2024-11-13-4.csv\")\n",
    "df_w1 = df_w1.query('`session.code` == \"3m87qmko\" | `session.code` == \"wt9ndgb1\"')\n",
    "# Fix the missing label\n",
    "df_w1.loc[df_w1[\"network_app.1.player.participantcode\"] == \"tfp\", \"network_app.1.player.participantcode\"] = \"tpf\"\n",
    "\n",
    "df_w2 = pd.read_csv(\"Raw/W2_all_apps_wide_2024-12-10-2.csv\")\n",
    "df_w2 = df_w2.query('`session.code` == \"2n8orvug\"')\n",
    "df_w2[\"participant.label\"][244] = \"nan\" # fixing the missing label\n",
    "\n",
    "df_w3 = pd.read_csv(\"Raw/W3_all_apps_wide_2025-01-29.csv\")\n",
    "df_w3 = df_w3.query('`session.code` == \"7uy8unkt\"')\n",
    "df_w3[\"participant.label\"][46] = \"nan\" # fixing the missing label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_function_w1(df):\n",
    "    # Drop rows where 'network_app.1.player.participantcode' is NaN\n",
    "    df = df.dropna(subset=['network_app.1.player.participantcode'])\n",
    "    # Lowercase the 'network_app.1.player.participantcode' column\n",
    "    df['network_app.1.player.participantcode'] = df['network_app.1.player.participantcode'].str.lower()\n",
    "\n",
    "\n",
    "    # Identify duplicates based on 'network_app.1.player.participantcode'\n",
    "    duplicates = df[df.duplicated(subset='network_app.1.player.participantcode', keep=False)]\n",
    "    # Filter duplicates where 'participant._index_in_pages' > 15\n",
    "    filtered_duplicates = duplicates[duplicates['participant._index_in_pages'] > 15]\n",
    "    # Remove the duplicates from the original dataframe (only those where 'participantcode' is duplicated)\n",
    "    df = df[~df['network_app.1.player.participantcode'].isin(duplicates['network_app.1.player.participantcode'])]\n",
    "    # Concatenate the filtered duplicates back into the dataframe\n",
    "    df = pd.concat([df, filtered_duplicates], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "    # Lowercase person columns ('network_app.1.player.person_1' to 'network_app.1.player.person_30')\n",
    "    person_columns = [f'network_app.1.player.person_{i}' for i in range(1, 31)]\n",
    "    df[person_columns] = df[person_columns].apply(lambda x: x.str.lower())\n",
    "    # Replace missing values with 'x' in all person columns\n",
    "    df[person_columns] = df[person_columns].fillna(\"x\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Other node and edge processing...\n",
    "        # Check if participantcode and person_1 match\n",
    "        if row[\"network_app.1.player.participantcode\"] == row[\"network_app.1.player.person_1\"]:\n",
    "            df.loc[_, \"network_app.1.player.linksrechts_self\"] = row[\"network_app.1.player.linksrechts_1\"]\n",
    "\n",
    "    df['participant.label'] = df['network_app.1.player.participantcode']\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_function_w2(df):\n",
    "    df = df.dropna(subset=['participant.label'])\n",
    "    \n",
    "    for i in range(1, 31):\n",
    "        df[f'network_app.1.player.person_{i}'] = df[f'network_app.1.player.person_{i}'].str.lower()\n",
    "\n",
    "    df['network_app.1.player.person_3'] = df['network_app.1.player.person_3'].str.replace(\"ny4\", \"ny3\", case=False, regex=False) #added because of commentary\n",
    "\n",
    "    # Replace missing values with \"x\" in columns: 'network_app.1.player.person_1', 'network_app.1.player.person_2' and 28 other columns\n",
    "    df = df.fillna({'network_app.1.player.person_1': \"x\", 'network_app.1.player.person_2': \"x\", 'network_app.1.player.person_3': \"x\", 'network_app.1.player.person_4': \"x\", 'network_app.1.player.person_5': \"x\", 'network_app.1.player.person_6': \"x\", 'network_app.1.player.person_7': \"x\", 'network_app.1.player.person_8': \"x\", 'network_app.1.player.person_9': \"x\", 'network_app.1.player.person_10': \"x\", 'network_app.1.player.person_11': \"x\", 'network_app.1.player.person_12': \"x\", 'network_app.1.player.person_13': \"x\", 'network_app.1.player.person_14': \"x\", 'network_app.1.player.person_15': \"x\", 'network_app.1.player.person_16': \"x\", 'network_app.1.player.person_17': \"x\", 'network_app.1.player.person_18': \"x\", 'network_app.1.player.person_19': \"x\", 'network_app.1.player.person_20': \"x\", 'network_app.1.player.person_21': \"x\", 'network_app.1.player.person_22': \"x\", 'network_app.1.player.person_23': \"x\", 'network_app.1.player.person_24': \"x\", 'network_app.1.player.person_25': \"x\", 'network_app.1.player.person_26': \"x\", 'network_app.1.player.person_27': \"x\", 'network_app.1.player.person_28': \"x\", 'network_app.1.player.person_29': \"x\", 'network_app.1.player.person_30': \"x\"})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_function_w3(df):\n",
    "    df = df.dropna(subset=['participant.label'])\n",
    "    \n",
    "    for i in range(1, 31):\n",
    "        df[f'network_app.1.player.person_{i}'] = df[f'network_app.1.player.person_{i}'].str.lower()\n",
    "\n",
    "    df['network_app.1.player.person_3'] = df['network_app.1.player.person_3'].str.replace(\"ny4\", \"ny3\", case=False, regex=False) #added because of commentary\n",
    "\n",
    "    # Replace missing values with \"x\" in columns: 'network_app.1.player.person_1', 'network_app.1.player.person_2' and 28 other columns\n",
    "    df = df.fillna({'network_app.1.player.person_1': \"x\", 'network_app.1.player.person_2': \"x\", 'network_app.1.player.person_3': \"x\", 'network_app.1.player.person_4': \"x\", 'network_app.1.player.person_5': \"x\", 'network_app.1.player.person_6': \"x\", 'network_app.1.player.person_7': \"x\", 'network_app.1.player.person_8': \"x\", 'network_app.1.player.person_9': \"x\", 'network_app.1.player.person_10': \"x\", 'network_app.1.player.person_11': \"x\", 'network_app.1.player.person_12': \"x\", 'network_app.1.player.person_13': \"x\", 'network_app.1.player.person_14': \"x\", 'network_app.1.player.person_15': \"x\", 'network_app.1.player.person_16': \"x\", 'network_app.1.player.person_17': \"x\", 'network_app.1.player.person_18': \"x\", 'network_app.1.player.person_19': \"x\", 'network_app.1.player.person_20': \"x\", 'network_app.1.player.person_21': \"x\", 'network_app.1.player.person_22': \"x\", 'network_app.1.player.person_23': \"x\", 'network_app.1.player.person_24': \"x\", 'network_app.1.player.person_25': \"x\", 'network_app.1.player.person_26': \"x\", 'network_app.1.player.person_27': \"x\", 'network_app.1.player.person_28': \"x\", 'network_app.1.player.person_29': \"x\", 'network_app.1.player.person_30': \"x\"})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_85829/32931137.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['network_app.1.player.participantcode'] = df['network_app.1.player.participantcode'].str.lower()\n",
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_85829/564018429.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'network_app.1.player.person_{i}'] = df[f'network_app.1.player.person_{i}'].str.lower()\n",
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_85829/564018429.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['network_app.1.player.person_3'] = df['network_app.1.player.person_3'].str.replace(\"ny4\", \"ny3\", case=False, regex=False) #added because of commentary\n",
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_85829/1171865204.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'network_app.1.player.person_{i}'] = df[f'network_app.1.player.person_{i}'].str.lower()\n",
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_85829/1171865204.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['network_app.1.player.person_3'] = df['network_app.1.player.person_3'].str.replace(\"ny4\", \"ny3\", case=False, regex=False) #added because of commentary\n"
     ]
    }
   ],
   "source": [
    "df_w1 = prep_function_w1(df_w1)\n",
    "df_w2 = prep_function_w2(df_w2)\n",
    "df_w3 = prep_function_w3(df_w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shiny_dataframe(df):\n",
    "    \"\"\"\n",
    "    Preprocesses the input DataFrame by performing column drops and replacing missing values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    # Columns to drop\n",
    "    drop_columns = [\n",
    "        'participant._is_bot', \n",
    "        'participant._index_in_pages', \n",
    "        'conjoint_app.1.player.participant_label', \n",
    "        'conjoint_app.1.player.id_in_group',\n",
    "        'participant._current_app_name', \n",
    "        'participant._current_page_name', \n",
    "        'conjoint_app.1.player.payoff', \n",
    "        'demographic_app.1.player.id_in_group',\n",
    "        'participant.visited', \n",
    "        'participant.mturk_worker_id', \n",
    "        'participant.mturk_assignment_id',\n",
    "        'participant.payoff', \n",
    "        'session.label', \n",
    "        'session.mturk_HITId', \n",
    "        'session.mturk_HITGroupId',\n",
    "        'session.comment', \n",
    "        'session.is_demo', \n",
    "        'session.config.real_world_currency_per_point',\n",
    "        'session.config.participation_fee', \n",
    "        'conjoint_app.1.player.language',\n",
    "        'conjoint_app.1.group.id_in_subsession', \n",
    "        'conjoint_app.1.subsession.round_number',\n",
    "        'demographic_app.1.player.role', \n",
    "        'demographic_app.1.player.payoff',\n",
    "        'network_app.1.player.role', \n",
    "        'network_app.1.player.payoff',\n",
    "        'demographic_app.1.group.id_in_subsession', \n",
    "        'demographic_app.1.subsession.round_number',\n",
    "        'conjoint_app.1.player.role', \n",
    "        'session.config.name',\n",
    "        'network_app.1.group.id_in_subsession', \n",
    "        'network_app.1.subsession.round_number',\n",
    "        'political_app.1.player.role', \n",
    "        'political_app.1.player.id_in_group',\n",
    "        'political_app.1.subsession.round_number', \n",
    "        'political_app.1.group.id_in_subsession',\n",
    "        'end_app.1.player.role', \n",
    "        'end_app.1.player.payoff', 'end_app.1.player.group_assignment',\n",
    "        'end_app.1.player.time_endpage', 'end_app.1.group.id_in_subsession',\n",
    "        'end_app.1.subsession.round_number', 'political_app.1.player.payoff',\n",
    "        'demographic_app.1.player.role',\n",
    "        'end_app.1.player.rnumber',\n",
    "        'end_app.1.player.rnumbercheck',\n",
    "        'conjoint_app.1.player.time_start',\n",
    "        'network_app.1.player.participantcode',\n",
    "    ]\n",
    "\n",
    "    # Columns with missing values to replace\n",
    "    fill_values_zero = {\n",
    "        'demographic_app.1.player.rent': -999,\n",
    "        'demographic_app.1.player.income': -999,\n",
    "        'participant.label': 0,\n",
    "        'political_app.1.player.sunday_poll': 0,\n",
    "        'political_app.1.player.sunday_party_vote': 0,\n",
    "        'political_app.1.player.sunday_not_eligible': 0,\n",
    "        'political_app.1.player.noteligible_sunday_party_vote': 0,\n",
    "        'demographic_app.1.player.social_networks_1': 0,\n",
    "        'demographic_app.1.player.social_networks_2': 0,\n",
    "        'demographic_app.1.player.social_networks_3': 0,\n",
    "        'demographic_app.1.player.social_networks_4': 0,\n",
    "        'demographic_app.1.player.social_networks_5': 0,\n",
    "        'demographic_app.1.player.social_networks_6': 0,\n",
    "        'demographic_app.1.player.social_networks_7': 0,\n",
    "        'demographic_app.1.player.social_networks_8': 0,\n",
    "        'demographic_app.1.player.social_networks_9': 0,\n",
    "        'demographic_app.1.player.social_networks_10': 0,\n",
    "        'demographic_app.1.player.participation_demonstration_1': 0,\n",
    "        'demographic_app.1.player.petition_signatory_1': 0,\n",
    "        'conjoint_app.1.player.participant_label': 0,\n",
    "        'network_app.1.player.participantcode': 0,\n",
    "        'political_app.1.player.reason_no_vote': 0,\n",
    "        'demographic_app.1.player.fresherscamp_student': 0,\n",
    "        'demographic_app.1.player.freshersweek_student': 0,\n",
    "        'demographic_app.1.player.school_father': 0,\n",
    "        'demographic_app.1.player.consecutive_study_program': 0,\n",
    "        'end_app.1.player.catdog': 0,\n",
    "        'demographic_app.1.player.work_edu_father': 0,\n",
    "    }\n",
    "\n",
    "    fill_values_string_zero = {\n",
    "        'demographic_app.1.player.social_networks_11': \"0\",\n",
    "        'demographic_app.1.player.study_program_other': \"0\"\n",
    "    }\n",
    "\n",
    "    fill_values_negative = {\n",
    "        'political_app.1.player.scalo_cdu': -999,\n",
    "        'political_app.1.player.scalo_csu': -999,\n",
    "        'political_app.1.player.scalo_spd': -999,\n",
    "        'political_app.1.player.scalo_gruene': -999,\n",
    "        'political_app.1.player.scalo_fdp': -999,\n",
    "        'political_app.1.player.scalo_afd': -999,\n",
    "        'political_app.1.player.scalo_linke': -999,\n",
    "        'political_app.1.player.scalo_bsw': -999,\n",
    "        'demographic_app.1.player.edu_mother': -999,\n",
    "        'demographic_app.1.player.study_program': -999\n",
    "    }\n",
    "\n",
    "    # Drop specified columns\n",
    "    df = df.drop(columns=[col for col in drop_columns if col in df.columns])\n",
    "\n",
    "    # # Replace missing values with 0\n",
    "    # df = df.fillna({col: val for col, val in fill_values_zero.items() if col in df.columns})\n",
    "    # # Replace missing values with \"0\"\n",
    "    # df = df.fillna({col: val for col, val in fill_values_string_zero.items() if col in df.columns})\n",
    "    # # Replace missing values with -999\n",
    "    # df = df.fillna({col: val for col, val in fill_values_negative.items() if col in df.columns})\n",
    "\n",
    "    # Inconsistency in no answer variable\n",
    "    df = df.replace(-888, -999)\n",
    "\n",
    "    # List of prefixes to keep\n",
    "    prefixes = ['conjoint', \n",
    "                'demographic', \n",
    "                'political', \n",
    "                'vignette', \n",
    "                'participant.label', \n",
    "                'network_app.1.player.participantcode',\n",
    "                'network_app.1.player.linksrechts_self',\n",
    "                #'network_app.1.player.person',\n",
    "                'end_app.1.player.catdog',\n",
    "                'end_app.1.player.rnumber']\n",
    "\n",
    "    # Filter columns by prefixes\n",
    "    df = df.loc[:, df.columns.str.startswith(tuple(prefixes))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_w1 = shiny_dataframe(df_w1)\n",
    "df_filtered_w2 = shiny_dataframe(df_w2)\n",
    "df_filtered_w3 = shiny_dataframe(df_w3)\n",
    "\n",
    "# Export dataframes as CSV\n",
    "df_filtered_w1.to_csv('Cooked/df_w1_prepared.csv', index=False)\n",
    "df_filtered_w2.to_csv('Cooked/df_w2_prepared.csv', index=False)\n",
    "df_filtered_w3.to_csv('Cooked/df_w3_prepared.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Waves for Complete Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(\n",
    "    df_filtered_w1,\n",
    "    df_filtered_w2,\n",
    "    how='outer',\n",
    "    on=\"participant.label\",\n",
    "    suffixes=('_W1', '_W2')\n",
    ")\n",
    "\n",
    "# Rename columns in df_filtered_w3\n",
    "df_filtered_w3 = df_filtered_w3.rename(columns=lambda x: f\"{x}_W3\" if x != \"participant.label\" else x)\n",
    "\n",
    "# Merge without suffixes\n",
    "merged_df = pd.merge(\n",
    "    merged_df,\n",
    "    df_filtered_w3,\n",
    "    how='outer',\n",
    "    on=\"participant.label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = merged_df.sort_index(axis=1)\n",
    "\n",
    "#complete_df.set_index('participant.label', inplace=True)\n",
    "\n",
    "complete_df = complete_df.drop(columns=['demographic_app.1.player.rent_W1','demographic_app.1.player.income_W1',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique participants across all DataFrames\n",
    "all_participants = set(df_w1['participant.label']) | set(df_w2['participant.label']) | set(df_w3['participant.label'])\n",
    "\n",
    "# Get participants present in all three DataFrames (intersection)\n",
    "complete = set(df_w1['participant.label']) & set(df_w2['participant.label']) & set(df_w3['participant.label'])\n",
    "\n",
    "# Find the participants that are in at least one DataFrame but NOT in all three\n",
    "difference = all_participants - complete\n",
    "\n",
    "# Filter the original DataFrames to get only rows that contain these \"difference\" participants\n",
    "df_w1_diff = df_w1[df_w1['participant.label'].isin(difference)]\n",
    "df_w2_diff = df_w2[df_w2['participant.label'].isin(difference)]\n",
    "df_w3_diff = df_w3[df_w3['participant.label'].isin(difference)]\n",
    "\n",
    "# Filter the DataFrames to keep only the matching rows\n",
    "df_w1_filtered = df_w1[df_w1['participant.label'].isin(complete)]\n",
    "df_w2_filtered = df_w2[df_w2['participant.label'].isin(complete)]\n",
    "df_w3_filtered = df_w3[df_w3['participant.label'].isin(complete)]\n",
    "\n",
    "df_diff = pd.merge(\n",
    "    df_w1_diff,\n",
    "    df_w2_diff,\n",
    "    how='outer',\n",
    "    on=\"participant.label\",\n",
    "    suffixes=('_W1', '_W2')\n",
    ")\n",
    "\n",
    "# Rename columns in df_filtered_w3\n",
    "df_w3_diff = df_w3_diff.rename(columns=lambda x: f\"{x}_W3\" if x != \"participant.label\" else x)\n",
    "\n",
    "# Merge without suffixes\n",
    "df_diff = pd.merge(\n",
    "    df_diff,\n",
    "    df_w3_diff,\n",
    "    how='outer',\n",
    "    on=\"participant.label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_85829/294661306.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  singles_df.sort_index(axis=1, inplace=True)\n",
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_85829/294661306.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  w2_w3_df.sort_index(axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "x = set(df_w1_diff['participant.label']) & set(df_w2_diff['participant.label'])\n",
    "y = set(df_w2_diff['participant.label']) & set(df_w3_diff['participant.label'])\n",
    "z = set(df_w1_diff['participant.label']) & set(df_w3_diff['participant.label'])\n",
    "\n",
    "comp = set(df_w1_diff['participant.label']) | set(df_w3_diff['participant.label']) | set(df_w2_diff['participant.label'])\n",
    "\n",
    "singles = comp - x - y - z \n",
    "\n",
    "w2_w3_df = df_diff[df_diff['participant.label'].isin(y)]\n",
    "singles_df = df_diff[df_diff['participant.label'].isin(singles)]\n",
    "singles_df.sort_index(axis=1, inplace=True)\n",
    "w2_w3_df.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_complete_df = complete_df[complete_df['participant.label'].isin(complete)]\n",
    "complete_complete_df.sort_index(axis=1, inplace=True)\n",
    "complete_complete_df.set_index('participant.label', inplace=True)\n",
    "#complete_complete_df.replace(-999, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the different waves with information from other waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_85829/275464716.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  complete_complete_df[current_col] = complete_complete_df[current_col].fillna(complete_complete_df[next_col])\n",
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_85829/275464716.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  complete_complete_df[next_col] = complete_complete_df[next_col].fillna(complete_complete_df[current_col])\n"
     ]
    }
   ],
   "source": [
    "# List of suffixes for waves\n",
    "suffixes = ['_W1', '_W2', '_W3']\n",
    "\n",
    "# Iterate over suffixes and fill missing values\n",
    "for i in range(len(suffixes) - 1):\n",
    "    current_suffix = suffixes[i]  # e.g., '_W1' or '_W2'\n",
    "    next_suffix = suffixes[i + 1]  # e.g., '_W2' or '_W3'\n",
    "\n",
    "    # Get columns for the current wave\n",
    "    current_columns = [col for col in complete_complete_df.columns if col.endswith(current_suffix)]\n",
    "\n",
    "    # For each column in the current wave, check if the corresponding column exists in the next wave\n",
    "    for current_col in current_columns:\n",
    "        next_col = current_col.replace(current_suffix, next_suffix)\n",
    "\n",
    "        # Fill missing values in the current wave column with values from the next wave column\n",
    "        if next_col in complete_complete_df.columns:\n",
    "            complete_complete_df[current_col] = complete_complete_df[current_col].fillna(complete_complete_df[next_col])\n",
    "\n",
    "        # Fill missing values in the next wave column with values from the current wave column\n",
    "        if current_col in complete_complete_df.columns and next_col in complete_complete_df.columns:\n",
    "            complete_complete_df[next_col] = complete_complete_df[next_col].fillna(complete_complete_df[current_col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_suffix(df, suffix):\n",
    "    # Filter columns that end with the specified suffix\n",
    "    columns = [col for col in df.columns if col.endswith(suffix)]\n",
    "    \n",
    "    # Create a subset DataFrame with the filtered columns\n",
    "    subset_df = df[columns]\n",
    "    \n",
    "    # Remove the suffix from the column names\n",
    "    subset_df.columns = [col.replace(suffix, '') for col in subset_df.columns]\n",
    "    \n",
    "    return subset_df\n",
    "\n",
    "# Example usage\n",
    "df_1 = split_df_by_suffix(complete_complete_df, '_W1')\n",
    "df_2 = split_df_by_suffix(complete_complete_df, '_W2')\n",
    "df_3 = split_df_by_suffix(complete_complete_df, '_W3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting idea would be two create new variables from multiple columns in the survey. \n",
    "- Popularity of political people \n",
    "- Socialmedia apps \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Network Dictionaries\n",
    "`dict_nodes = {participant_label : {participant_attributes : values, network_app.1.player.person_1...30 : {Friends : True, Politics : True}}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing_with_default(value, default_value=np.nan):\n",
    "    if pd.isna(value) or value in [\"\"]:\n",
    "        return default_value\n",
    "    return value\n",
    "\n",
    "def create_nodes(df, unique_id, wave):\n",
    "    # Initialize nodes with empty dictionaries\n",
    "    dict_nodes = {label: {} for label in df[unique_id]}\n",
    "\n",
    "    # Iterate through rows to populate node attributes\n",
    "    for _, row in df.iterrows():\n",
    "        label = row[unique_id]\n",
    "        if label in dict_nodes:\n",
    "            # Add all columns to the dictionary\n",
    "            for column in df.columns:\n",
    "                # Skip the unique_id column to avoid redundancy\n",
    "                if column == unique_id:\n",
    "                    continue\n",
    "                # Extract the part after the last dot\n",
    "                key = column.split('.')[-1]\n",
    "                # Remove the underscore and everything after it\n",
    "                #key = key.split('_')[0]\n",
    "                # Use the simplified key and replace missing values\n",
    "                #dict_nodes[label][key] = replace_missing_with_default(row[column])\n",
    "\n",
    "    return dict_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edges(df, unique_id, dict_nodes):\n",
    "    dict_edges = {label: {} for label in df[unique_id]}\n",
    "    dropped = 0\n",
    "    list_dropped = []\n",
    "    edge_info = {\n",
    "        \"friend\": 0,\n",
    "        \"value\": 0,\n",
    "        \"politics\": 0,\n",
    "        \"study\": 0,\n",
    "        \"council\": 0,\n",
    "        \"leftright\": 0,\n",
    "        \"sentiment\": 0,\n",
    "        \"aquaintance\": 0\n",
    "    }\n",
    "\n",
    "    # Iterate through rows to populate edge attributes\n",
    "    for _, row in df.iterrows():\n",
    "        label = row[unique_id]\n",
    "        for i in range(1, 30):\n",
    "            target_person = row[f\"network_app.1.player.person_{i}\"]\n",
    "            if target_person != \"x\" and target_person != label:\n",
    "                if target_person in dict_nodes:\n",
    "                    dict_edges[label][target_person] = {\n",
    "                        \"aquaintance\": True,\n",
    "                        \"friend\": replace_missing_with_default(row[f\"network_app.1.player.friend_{i}\"]) == 1,\n",
    "                        \"value\": replace_missing_with_default(row[f\"network_app.1.player.value_{i}\"]) == 1,\n",
    "                        \"politics\": replace_missing_with_default(row[f\"network_app.1.player.politics_{i}\"]) == 1,\n",
    "                        \"study\": replace_missing_with_default(row[f\"network_app.1.player.study_{i}\"]) == 1,\n",
    "                        \"council\": replace_missing_with_default(row[f\"network_app.1.player.council_{i}\"]) == 1,\n",
    "                        \"leftright\": replace_missing_with_default(row[f\"network_app.1.player.linksrechts_{i}\"])\n",
    "                    }\n",
    "                    try:\n",
    "                        dict_edges[label][target_person][\"sentiment\"] = replace_missing_with_default(row[f\"network_app.1.player.sentiment_{i}\"])\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "\n",
    "                    # Update edge information counts\n",
    "                    edge_info[\"friend\"] += dict_edges[label][target_person][\"friend\"]\n",
    "                    edge_info[\"value\"] += dict_edges[label][target_person][\"value\"]\n",
    "                    edge_info[\"politics\"] += dict_edges[label][target_person][\"politics\"]\n",
    "                    edge_info[\"study\"] += dict_edges[label][target_person][\"study\"]\n",
    "                    edge_info[\"council\"] += dict_edges[label][target_person][\"council\"]\n",
    "                    edge_info[\"leftright\"] += 1  # Assuming each edge has a leftright value\n",
    "                    if \"sentiment\" in dict_edges[label][target_person]:\n",
    "                        edge_info[\"sentiment\"] += 1\n",
    "                else:\n",
    "                    # Log dropped connections\n",
    "                    dropped += 1\n",
    "                    list_dropped.append(target_person)\n",
    "\n",
    "    return dict_edges, edge_info, list_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extraction(df_nodes, df_edges, unique_id, wave):\n",
    "    # Create nodes\n",
    "    dict_nodes = create_nodes(df_nodes, unique_id, wave)\n",
    "    \n",
    "    # Create edges\n",
    "    dict_edges, edge_info, list_dropped = create_edges(df_edges, unique_id, dict_nodes)\n",
    "\n",
    "    # Save nodes and edges to JSON files\n",
    "    with open(f'NA/nodes_{wave}.json', 'w') as json_file:\n",
    "        json.dump(dict_nodes, json_file, indent=4)\n",
    "\n",
    "    with open(f'NA/edges_{wave}.json', 'w') as json_file:\n",
    "        json.dump(dict_edges, json_file, indent=4)\n",
    "\n",
    "    # Print edge information summary\n",
    "    print(f\"Edge information for wave {wave}:\")\n",
    "    for key, value in edge_info.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    sum_edges = sum(len(edges) for edges in dict_edges.values())\n",
    "    print(\"Amount of Edges dropped:\", len(list_dropped))\n",
    "\n",
    "    return dict_nodes, dict_edges, list_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge information for wave W1:\n",
      "friend: 338\n",
      "value: 253\n",
      "politics: 228\n",
      "study: 260\n",
      "council: 327\n",
      "leftright: 599\n",
      "sentiment: 0\n",
      "aquaintance: 0\n",
      "Amount of Edges dropped: 79\n",
      "Edge information for wave W2:\n",
      "friend: 361\n",
      "value: 235\n",
      "politics: 210\n",
      "study: 187\n",
      "council: 317\n",
      "leftright: 589\n",
      "sentiment: 589\n",
      "aquaintance: 0\n",
      "Amount of Edges dropped: 88\n",
      "Edge information for wave W3:\n",
      "friend: 352\n",
      "value: 262\n",
      "politics: 219\n",
      "study: 195\n",
      "council: 338\n",
      "leftright: 601\n",
      "sentiment: 601\n",
      "aquaintance: 0\n",
      "Amount of Edges dropped: 53\n"
     ]
    }
   ],
   "source": [
    "df_1 = df_1.reset_index()\n",
    "df_2 = df_2.reset_index()\n",
    "df_3 = df_3.reset_index()\n",
    "\n",
    "dict_nodes_w1, dict_edges_w1, list_dropped_w1 = data_extraction(df_1, df_w1_filtered ,'participant.label', \"W1\")\n",
    "dict_nodes_w2, dict_edges_w2, list_dropped_w2 = data_extraction(df_2, df_w2_filtered, \"participant.label\", \"W2\")\n",
    "dict_nodes_w3, dict_edges_w3, list_dropped_w3 = data_extraction(df_3, df_w3_filtered, \"participant.label\", \"W3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_venv_3.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
