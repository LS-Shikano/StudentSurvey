{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_74493/3701200836.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_w2[\"participant.label\"][244] = \"nan\" # fixing the missing label\n"
     ]
    }
   ],
   "source": [
    "df_w1 = pd.read_csv(\"Raw/W1_AllAppsWide_2024-11-13-4.csv\")\n",
    "df_w1 = df_w1.query('`session.code` == \"3m87qmko\" | `session.code` == \"wt9ndgb1\"')\n",
    "\n",
    "df_w2 = pd.read_csv(\"Raw/W2_all_apps_wide_2024-12-10-2.csv\")\n",
    "df_w2 = df_w2.query('`session.code` == \"2n8orvug\"')\n",
    "df_w2[\"participant.label\"][244] = \"nan\" # fixing the missing label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- drop columns that are not needed \n",
    "- drop missing values for unique-ID column \n",
    "- check for duplicates in unique-ID and decide what to do with them \n",
    "    - keep them based on finishing survey, so last page visited\n",
    "- prepare networks and fill na with x\n",
    "- drop columns based on prefixes, or keep columns based on prefixes of apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_function_w1(df):\n",
    "    # Drop rows where 'network_app.1.player.participantcode' is NaN\n",
    "    df = df.dropna(subset=['network_app.1.player.participantcode'])\n",
    "    # Lowercase the 'network_app.1.player.participantcode' column\n",
    "    df['network_app.1.player.participantcode'] = df['network_app.1.player.participantcode'].str.lower()\n",
    "\n",
    "\n",
    "    # Identify duplicates based on 'network_app.1.player.participantcode'\n",
    "    duplicates = df[df.duplicated(subset='network_app.1.player.participantcode', keep=False)]\n",
    "    # Filter duplicates where 'participant._index_in_pages' > 15\n",
    "    filtered_duplicates = duplicates[duplicates['participant._index_in_pages'] > 15]\n",
    "    # Remove the duplicates from the original dataframe (only those where 'participantcode' is duplicated)\n",
    "    df = df[~df['network_app.1.player.participantcode'].isin(duplicates['network_app.1.player.participantcode'])]\n",
    "    # Concatenate the filtered duplicates back into the dataframe\n",
    "    df = pd.concat([df, filtered_duplicates], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "    # Lowercase person columns ('network_app.1.player.person_1' to 'network_app.1.player.person_30')\n",
    "    person_columns = [f'network_app.1.player.person_{i}' for i in range(1, 31)]\n",
    "    df[person_columns] = df[person_columns].apply(lambda x: x.str.lower())\n",
    "    # Replace missing values with 'x' in all person columns\n",
    "    df[person_columns] = df[person_columns].fillna(\"x\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_function_w2(df):\n",
    "    df = df.dropna(subset=['participant.label'])\n",
    "    \n",
    "    for i in range(1, 31):\n",
    "        df[f'network_app.1.player.person_{i}'] = df[f'network_app.1.player.person_{i}'].str.lower()\n",
    "\n",
    "    # Replace missing values with \"x\" in columns: 'network_app.1.player.person_1', 'network_app.1.player.person_2' and 28 other columns\n",
    "    df = df.fillna({'network_app.1.player.person_1': \"x\", 'network_app.1.player.person_2': \"x\", 'network_app.1.player.person_3': \"x\", 'network_app.1.player.person_4': \"x\", 'network_app.1.player.person_5': \"x\", 'network_app.1.player.person_6': \"x\", 'network_app.1.player.person_7': \"x\", 'network_app.1.player.person_8': \"x\", 'network_app.1.player.person_9': \"x\", 'network_app.1.player.person_10': \"x\", 'network_app.1.player.person_11': \"x\", 'network_app.1.player.person_12': \"x\", 'network_app.1.player.person_13': \"x\", 'network_app.1.player.person_14': \"x\", 'network_app.1.player.person_15': \"x\", 'network_app.1.player.person_16': \"x\", 'network_app.1.player.person_17': \"x\", 'network_app.1.player.person_18': \"x\", 'network_app.1.player.person_19': \"x\", 'network_app.1.player.person_20': \"x\", 'network_app.1.player.person_21': \"x\", 'network_app.1.player.person_22': \"x\", 'network_app.1.player.person_23': \"x\", 'network_app.1.player.person_24': \"x\", 'network_app.1.player.person_25': \"x\", 'network_app.1.player.person_26': \"x\", 'network_app.1.player.person_27': \"x\", 'network_app.1.player.person_28': \"x\", 'network_app.1.player.person_29': \"x\", 'network_app.1.player.person_30': \"x\"})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shiny_dataframe(df):\n",
    "    \"\"\"\n",
    "    Preprocesses the input DataFrame by performing column drops and replacing missing values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    # Columns to drop\n",
    "    drop_columns = [\n",
    "        'participant._is_bot', 'participant._index_in_pages',\n",
    "        'participant._current_app_name', 'participant._current_page_name',\n",
    "        'participant.visited', 'participant.mturk_worker_id', 'participant.mturk_assignment_id',\n",
    "        'participant.payoff', 'session.label', 'session.mturk_HITId', 'session.mturk_HITGroupId',\n",
    "        'session.comment', 'session.is_demo', 'session.config.real_world_currency_per_point',\n",
    "        'session.config.participation_fee', 'conjoint_app.1.player.language',\n",
    "        'conjoint_app.1.group.id_in_subsession', 'conjoint_app.1.subsession.round_number',\n",
    "        'demographic_app.1.player.role', 'demographic_app.1.player.payoff',\n",
    "        'network_app.1.player.role', 'network_app.1.player.payoff',\n",
    "        'demographic_app.1.group.id_in_subsession', 'demographic_app.1.subsession.round_number',\n",
    "        'conjoint_app.1.player.role', 'session.config.name',\n",
    "        'network_app.1.group.id_in_subsession', 'network_app.1.subsession.round_number',\n",
    "        'political_app.1.player.role', 'political_app.1.player.id_in_group',\n",
    "        'political_app.1.subsession.round_number', 'political_app.1.group.id_in_subsession',\n",
    "        'end_app.1.player.role', 'end_app.1.player.payoff', 'end_app.1.player.group_assignment',\n",
    "        'end_app.1.player.time_endpage', 'end_app.1.group.id_in_subsession',\n",
    "        'end_app.1.subsession.round_number', 'political_app.1.player.payoff'\n",
    "    ]\n",
    "\n",
    "    # Columns with missing values to replace\n",
    "    fill_values_zero = {\n",
    "        'demographic_app.1.player.rent': 0,\n",
    "        'demographic_app.1.player.income': 0,\n",
    "        'participant.label': 0,\n",
    "        'political_app.1.player.sunday_poll': 0,\n",
    "        'political_app.1.player.sunday_party_vote': 0,\n",
    "        'political_app.1.player.sunday_not_eligible': 0,\n",
    "        'political_app.1.player.noteligible_sunday_party_vote': 0,\n",
    "        'demographic_app.1.player.social_networks_1': 0,\n",
    "        'demographic_app.1.player.social_networks_2': 0,\n",
    "        'demographic_app.1.player.social_networks_3': 0,\n",
    "        'demographic_app.1.player.social_networks_4': 0,\n",
    "        'demographic_app.1.player.social_networks_5': 0,\n",
    "        'demographic_app.1.player.social_networks_6': 0,\n",
    "        'demographic_app.1.player.social_networks_7': 0,\n",
    "        'demographic_app.1.player.social_networks_8': 0,\n",
    "        'demographic_app.1.player.social_networks_9': 0,\n",
    "        'demographic_app.1.player.social_networks_10': 0,\n",
    "        'demographic_app.1.player.participation_demonstration_1': 0,\n",
    "        'demographic_app.1.player.petition_signatory_1': 0,\n",
    "        'conjoint_app.1.player.participant_label': 0,\n",
    "        'network_app.1.player.participantcode': 0,\n",
    "        'political_app.1.player.reason_no_vote': 0\n",
    "    }\n",
    "\n",
    "    fill_values_string_zero = {\n",
    "        'demographic_app.1.player.social_networks_11': \"0\",\n",
    "        'demographic_app.1.player.study_program_other': \"0\"\n",
    "    }\n",
    "\n",
    "    fill_values_negative = {\n",
    "        'political_app.1.player.scalo_cdu': -999,\n",
    "        'political_app.1.player.scalo_csu': -999,\n",
    "        'political_app.1.player.scalo_spd': -999,\n",
    "        'political_app.1.player.scalo_gruene': -999,\n",
    "        'political_app.1.player.scalo_fdp': -999,\n",
    "        'political_app.1.player.scalo_afd': -999,\n",
    "        'political_app.1.player.scalo_linke': -999,\n",
    "        'political_app.1.player.scalo_bsw': -999,\n",
    "        'demographic_app.1.player.edu_mother': -999,\n",
    "        'demographic_app.1.player.study_program': -999\n",
    "    }\n",
    "\n",
    "    # Drop specified columns\n",
    "    df = df.drop(columns=[col for col in drop_columns if col in df.columns])\n",
    "\n",
    "    # Replace missing values with 0\n",
    "    df = df.fillna({col: val for col, val in fill_values_zero.items() if col in df.columns})\n",
    "\n",
    "    # Replace missing values with \"0\"\n",
    "    df = df.fillna({col: val for col, val in fill_values_string_zero.items() if col in df.columns})\n",
    "\n",
    "    # Replace missing values with -999\n",
    "    df = df.fillna({col: val for col, val in fill_values_negative.items() if col in df.columns})\n",
    "\n",
    "    # Inconsistency in no answer variable\n",
    "    df = df.replace(-888, -999)\n",
    "\n",
    "    # List of prefixes to keep\n",
    "    prefixes = ['conjoint', \n",
    "                'demographic', \n",
    "                'political', \n",
    "                'vignette', \n",
    "                'participant.label', \n",
    "                'network_app.1.player.participantcode']\n",
    "\n",
    "    # Filter columns by prefixes\n",
    "    df = df.loc[:, df.columns.str.startswith(tuple(prefixes))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_74493/1119181786.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['network_app.1.player.participantcode'] = df['network_app.1.player.participantcode'].str.lower()\n",
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_74493/1194046896.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'network_app.1.player.person_{i}'] = df[f'network_app.1.player.person_{i}'].str.lower()\n"
     ]
    }
   ],
   "source": [
    "df_w1 = prep_function_w1(df_w1)\n",
    "df_w2 = prep_function_w2(df_w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_74493/2247390915.py:80: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna({col: val for col, val in fill_values_zero.items() if col in df.columns})\n"
     ]
    }
   ],
   "source": [
    "df_filtered_w1 = shiny_dataframe(df_w1)\n",
    "df_filtered_w2 = shiny_dataframe(df_w2)\n",
    "\n",
    "# Export dataframes as CSV\n",
    "df_filtered_w1.to_csv('Cooked/df_w1_prepared.csv', index=False)\n",
    "df_filtered_w2.to_csv('Cooked/df_w2_prepared.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 180 180\n"
     ]
    }
   ],
   "source": [
    "a = set(df_w1['network_app.1.player.participantcode']) \n",
    "b = set(df_w2['participant.label'])\n",
    "\n",
    "c = a.intersection(b)\n",
    "\n",
    "print(len(c),len(a),len(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teilnehmer: w1:180, w2:180\n",
    "\n",
    "Schnittmenge Wellen: 163 (-17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Network Dictionaries\n",
    "`dict_nodes = {participant_label : {participant_attributes : values, network_app.1.player.person_1...30 : {Friends : True, Politics : True}}}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We loose a lot of observation because of wrong entries for participantcodes of aquaintances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def replace_missing_with_default(value, default_value=-999):\n",
    "    \"\"\"\n",
    "    Replace missing values (None, NaN, or empty) with a default value.\n",
    "    \"\"\"\n",
    "    if pd.isna(value) or value == \"\" or value is None:\n",
    "        return default_value\n",
    "    return value\n",
    "\n",
    "def data_extraction(df, unique_id, wave):\n",
    "    # Initialize nodes with empty dictionaries\n",
    "    dict_nodes = {label: {} for label in df[unique_id]}\n",
    "    dict_edges = {label: {} for label in df[unique_id]}\n",
    "    dropped = 0\n",
    "    list_dropped = []\n",
    "    edge_info = {\n",
    "        \"friend\": 0,\n",
    "        \"value\": 0,\n",
    "        \"politics\": 0,\n",
    "        \"study\": 0,\n",
    "        \"council\": 0,\n",
    "        \"leftright\": 0,\n",
    "        \"sentiment\": 0\n",
    "    }\n",
    "\n",
    "    # Iterate through rows to populate node and edge attributes\n",
    "    for _, row in df.iterrows():\n",
    "        # Add node-specific attributes\n",
    "        label = row[unique_id]\n",
    "        if label in dict_nodes:\n",
    "            dict_nodes[label][\"leftrightself\"] = replace_missing_with_default(row[\"network_app.1.player.linksrechts_self\"])\n",
    "            dict_nodes[label][\"gender\"] = replace_missing_with_default(row[\"demographic_app.1.player.gender\"])\n",
    "            dict_nodes[label][\"income\"] = replace_missing_with_default(row[\"demographic_app.1.player.income\"])\n",
    "            dict_nodes[label][\"rent\"] = replace_missing_with_default(row[\"demographic_app.1.player.rent\"])\n",
    "            dict_nodes[label][\"grade\"] = replace_missing_with_default(row[\"demographic_app.1.player.grade\"])\n",
    "            dict_nodes[label][\"partyvote\"] = replace_missing_with_default(row['political_app.1.player.sunday_party_vote'])\n",
    "            dict_nodes[label][\"age\"] = replace_missing_with_default(row['demographic_app.1.player.age'])\n",
    "            dict_nodes[label][\"ocu_father\"] = replace_missing_with_default(row['demographic_app.1.player.ocu_father'])\n",
    "            dict_nodes[label][\"ocu_mother\"] = replace_missing_with_default(row['demographic_app.1.player.ocu_mother'])\n",
    "            dict_nodes[label][\"edu_father\"] = replace_missing_with_default(row['demographic_app.1.player.edu_father'])\n",
    "            dict_nodes[label][\"edz_mother\"] = replace_missing_with_default(row['demographic_app.1.player.edu_mother'])        \n",
    "            # add more attributes here\n",
    "\n",
    "        # Process connections and add edge-specific attributes\n",
    "        for i in range(1, 30):\n",
    "            target_person = row[f\"network_app.1.player.person_{i}\"]\n",
    "            if target_person != \"x\" and target_person != label:\n",
    "                if target_person in dict_nodes:\n",
    "                    dict_edges[label][target_person] = {\n",
    "                        \"friend\": replace_missing_with_default(row[f\"network_app.1.player.friend_{i}\"]) == 1,\n",
    "                        \"value\": replace_missing_with_default(row[f\"network_app.1.player.value_{i}\"]) == 1,\n",
    "                        \"politics\": replace_missing_with_default(row[f\"network_app.1.player.politics_{i}\"]) == 1,\n",
    "                        \"study\": replace_missing_with_default(row[f\"network_app.1.player.study_{i}\"]) == 1,\n",
    "                        \"council\": replace_missing_with_default(row[f\"network_app.1.player.council_{i}\"]) == 1,\n",
    "                        \"leftright\": replace_missing_with_default(row[f\"network_app.1.player.linksrechts_{i}\"])\n",
    "                    }\n",
    "                    try:\n",
    "                        dict_edges[label][target_person][\"sentiment\"] = replace_missing_with_default(row[f\"network_app.1.player.sentiment_{i}\"])\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "\n",
    "                    # Update edge information counts\n",
    "                    edge_info[\"friend\"] += dict_edges[label][target_person][\"friend\"]\n",
    "                    edge_info[\"value\"] += dict_edges[label][target_person][\"value\"]\n",
    "                    edge_info[\"politics\"] += dict_edges[label][target_person][\"politics\"]\n",
    "                    edge_info[\"study\"] += dict_edges[label][target_person][\"study\"]\n",
    "                    edge_info[\"council\"] += dict_edges[label][target_person][\"council\"]\n",
    "                    edge_info[\"leftright\"] += 1  # Assuming each edge has a leftright value\n",
    "                    if \"sentiment\" in dict_edges[label][target_person]:\n",
    "                        edge_info[\"sentiment\"] += 1\n",
    "                else:\n",
    "                    # Log dropped connections\n",
    "                    dropped += 1\n",
    "                    list_dropped.append(target_person)\n",
    "\n",
    "    # Save nodes and edges to JSON files\n",
    "    with open(f'NA/nodes_{wave}.json', 'w') as json_file:\n",
    "        json.dump(dict_nodes, json_file, indent=4)\n",
    "\n",
    "    with open(f'NA/edges_{wave}.json', 'w') as json_file:\n",
    "        json.dump(dict_edges, json_file, indent=4)\n",
    "\n",
    "    # Print edge information summary\n",
    "    print(f\"Edge information for wave {wave}:\")\n",
    "    for key, value in edge_info.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    sum=0\n",
    "    for key,value in dict_edges.items():\n",
    "        #print(key)\n",
    "        sum += len(value)\n",
    "    print(\"Amount of Edges dropped:\", len(list_dropped))\n",
    "\n",
    "    return dict_nodes, dict_edges, list_dropped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge information for wave W2:\n",
      "friend: 436\n",
      "value: 282\n",
      "politics: 232\n",
      "study: 226\n",
      "council: 368\n",
      "leftright: 719\n",
      "sentiment: 719\n",
      "Amount of Edges dropped: 55\n",
      "Edge information for wave W1:\n",
      "friend: 375\n",
      "value: 277\n",
      "politics: 242\n",
      "study: 282\n",
      "council: 367\n",
      "leftright: 677\n",
      "sentiment: 0\n",
      "Amount of Edges dropped: 87\n"
     ]
    }
   ],
   "source": [
    "dict_nodes_w2, dict_edges_w2, list_dropped_w2 = data_extraction(df_w2, \"participant.label\", \"W2\")\n",
    "dict_nodes_w1, dict_edges_w1, list_dropped_w1 = data_extraction(df_w1, 'network_app.1.player.participantcode', \"W1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_venv_3.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
