{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1 = pd.read_csv(\"Raw/W1_AllAppsWide_2024-11-13-4.csv\")\n",
    "df_w1 = df_w1.query('`session.code` == \"3m87qmko\" | `session.code` == \"wt9ndgb1\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_2407/1927908836.py:3: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_w2[\"participant.label\"][244] = \"nan\" # fixing the missing label\n"
     ]
    }
   ],
   "source": [
    "df_w2 = pd.read_csv(\"Raw/W2_all_apps_wide_2024-12-10-2.csv\")\n",
    "df_w2 = df_w2.query('`session.code` == \"2n8orvug\"')\n",
    "df_w2[\"participant.label\"][244] = \"nan\" # fixing the missing label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- drop columns that are not needed \n",
    "- drop missing values for unique-ID column \n",
    "- check for duplicates in unique-ID and decide what to do with them \n",
    "    - keep them based on finishing survey, so last page visited\n",
    "- prepare networks and fill na with x\n",
    "- drop columns based on prefixes, or keep columns based on prefixes of apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_function_w1(df):\n",
    "    # Drop rows where 'network_app.1.player.participantcode' is NaN\n",
    "    df = df.dropna(subset=['network_app.1.player.participantcode'])\n",
    "    # Lowercase the 'network_app.1.player.participantcode' column\n",
    "    df['network_app.1.player.participantcode'] = df['network_app.1.player.participantcode'].str.lower()\n",
    "\n",
    "\n",
    "    # Identify duplicates based on 'network_app.1.player.participantcode'\n",
    "    duplicates = df[df.duplicated(subset='network_app.1.player.participantcode', keep=False)]\n",
    "    # Filter duplicates where 'participant._index_in_pages' > 15\n",
    "    filtered_duplicates = duplicates[duplicates['participant._index_in_pages'] > 15]\n",
    "    # Remove the duplicates from the original dataframe (only those where 'participantcode' is duplicated)\n",
    "    df = df[~df['network_app.1.player.participantcode'].isin(duplicates['network_app.1.player.participantcode'])]\n",
    "    # Concatenate the filtered duplicates back into the dataframe\n",
    "    df = pd.concat([df, filtered_duplicates], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "    # Lowercase person columns ('network_app.1.player.person_1' to 'network_app.1.player.person_30')\n",
    "    person_columns = [f'network_app.1.player.person_{i}' for i in range(1, 31)]\n",
    "    df[person_columns] = df[person_columns].apply(lambda x: x.str.lower())\n",
    "    # Replace missing values with 'x' in all person columns\n",
    "    df[person_columns] = df[person_columns].fillna(\"x\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_function_w2(df):\n",
    "    df = df.dropna(subset=['participant.label'])\n",
    "    \n",
    "    for i in range(1, 31):\n",
    "        df[f'network_app.1.player.person_{i}'] = df[f'network_app.1.player.person_{i}'].str.lower()\n",
    "\n",
    "    # Replace missing values with \"x\" in columns: 'network_app.1.player.person_1', 'network_app.1.player.person_2' and 28 other columns\n",
    "    df = df.fillna({'network_app.1.player.person_1': \"x\", 'network_app.1.player.person_2': \"x\", 'network_app.1.player.person_3': \"x\", 'network_app.1.player.person_4': \"x\", 'network_app.1.player.person_5': \"x\", 'network_app.1.player.person_6': \"x\", 'network_app.1.player.person_7': \"x\", 'network_app.1.player.person_8': \"x\", 'network_app.1.player.person_9': \"x\", 'network_app.1.player.person_10': \"x\", 'network_app.1.player.person_11': \"x\", 'network_app.1.player.person_12': \"x\", 'network_app.1.player.person_13': \"x\", 'network_app.1.player.person_14': \"x\", 'network_app.1.player.person_15': \"x\", 'network_app.1.player.person_16': \"x\", 'network_app.1.player.person_17': \"x\", 'network_app.1.player.person_18': \"x\", 'network_app.1.player.person_19': \"x\", 'network_app.1.player.person_20': \"x\", 'network_app.1.player.person_21': \"x\", 'network_app.1.player.person_22': \"x\", 'network_app.1.player.person_23': \"x\", 'network_app.1.player.person_24': \"x\", 'network_app.1.player.person_25': \"x\", 'network_app.1.player.person_26': \"x\", 'network_app.1.player.person_27': \"x\", 'network_app.1.player.person_28': \"x\", 'network_app.1.player.person_29': \"x\", 'network_app.1.player.person_30': \"x\"})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_2407/1119181786.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['network_app.1.player.participantcode'] = df['network_app.1.player.participantcode'].str.lower()\n",
      "/var/folders/jl/tj67gv9n4_sd4yxc6ynl78500000gn/T/ipykernel_2407/1194046896.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'network_app.1.player.person_{i}'] = df[f'network_app.1.player.person_{i}'].str.lower()\n"
     ]
    }
   ],
   "source": [
    "df_w1 = prep_function_w1(df_w1)\n",
    "df_w2 = prep_function_w2(df_w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of prefixes to keep\n",
    "prefixes = ['conjoint', 'demographic', 'political', 'vignette', 'participant.label', 'network_app.1.player.participantcode']\n",
    "\n",
    "# Filter columns by prefixes\n",
    "df_filtered_w2 = df_w2.loc[:, df_w2.columns.str.startswith(tuple(prefixes))]\n",
    "df_filtered_w1 = df_w1.loc[:, df_w1.columns.str.startswith(tuple(prefixes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframes as CSV\n",
    "df_filtered_w1.to_csv('df_w1_prepared.csv', index=False)\n",
    "df_filtered_w2.to_csv('df_w2_prepared.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set(df_w1['network_app.1.player.participantcode']) \n",
    "b = set(df_w2['participant.label'])\n",
    "\n",
    "c = a.intersection(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 180 180\n"
     ]
    }
   ],
   "source": [
    "print(len(c),len(a),len(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teilnehmer: w1:180, w2:180\n",
    "\n",
    "Schnittmenge Wellen: 163 (-17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_w1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_w1\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_w1' is not defined"
     ]
    }
   ],
   "source": [
    "df_w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Dictionaries\n",
    "`dict_nodes = {participant_label : {participant_attributes : values, network_app.1.player.person_1...30 : {Friends : True, Politics : True}}}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We loose a lot of observation because of wrong entries for participantcodes of aquaintances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extraction(df, unique_id, wave):\n",
    "    # Initialize nodes with empty dictionaries\n",
    "    dict_nodes = {label: {} for label in df[unique_id]}\n",
    "    dict_edges = {label: {} for label in df[unique_id]}\n",
    "    dropped = 0\n",
    "    list_dropped = []\n",
    "    edge_info = {\n",
    "        \"friend\": 0,\n",
    "        \"value\": 0,\n",
    "        \"politics\": 0,\n",
    "        \"study\": 0,\n",
    "        \"council\": 0,\n",
    "        \"leftright\": 0,\n",
    "        \"sentiment\": 0\n",
    "    }\n",
    "\n",
    "    # Iterate through rows to populate node and edge attributes\n",
    "    for _, row in df.iterrows():\n",
    "        # Add node-specific attributes\n",
    "        label = row[unique_id]\n",
    "        if label in dict_nodes:\n",
    "            dict_nodes[label][\"leftrightself\"] = row[\"network_app.1.player.linksrechts_self\"]\n",
    "            dict_nodes[label][\"gender\"] = row[\"demographic_app.1.player.gender\"]\n",
    "            dict_nodes[label][\"income\"] = row[\"demographic_app.1.player.income\"]\n",
    "            dict_nodes[label][\"rent\"] = row[\"demographic_app.1.player.rent\"]\n",
    "            dict_nodes[label][\"grade\"] = row[\"demographic_app.1.player.grade\"]\n",
    "            dict_nodes[label][\"partyvote\"] = row['political_app.1.player.sunday_party_vote']\n",
    "            dict_nodes[label][\"age\"] = row['demographic_app.1.player.age']\n",
    "            dict_nodes[label][\"ocu_father\"] = row['demographic_app.1.player.ocu_father']\n",
    "            dict_nodes[label][\"ocu_mother\"] = row['demographic_app.1.player.ocu_mother']\n",
    "            dict_nodes[label][\"edu_father\"] = row['demographic_app.1.player.edu_father']\n",
    "            dict_nodes[label][\"edz_mother\"] = row['demographic_app.1.player.edu_mother']        \n",
    "            # add more attributes here\n",
    "\n",
    "        # Process connections and add edge-specific attributes\n",
    "        for i in range(1, 30):\n",
    "            target_person = row[f\"network_app.1.player.person_{i}\"]\n",
    "            if target_person != \"x\" and target_person != label:\n",
    "                if target_person in dict_nodes:\n",
    "                    dict_edges[label][target_person] = {\n",
    "                        \"friend\": row[f\"network_app.1.player.friend_{i}\"] == 1,\n",
    "                        \"value\": row[f\"network_app.1.player.value_{i}\"] == 1,\n",
    "                        \"politics\": row[f\"network_app.1.player.politics_{i}\"] == 1,\n",
    "                        \"study\": row[f\"network_app.1.player.study_{i}\"] == 1,\n",
    "                        \"council\": row[f\"network_app.1.player.council_{i}\"] == 1,\n",
    "                        \"leftright\": row[f\"network_app.1.player.linksrechts_{i}\"]\n",
    "                    }\n",
    "                    try:\n",
    "                        dict_edges[label][target_person][\"sentiment\"] = row[f\"network_app.1.player.sentiment_{i}\"]\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "\n",
    "                    # Update edge information counts\n",
    "                    edge_info[\"friend\"] += dict_edges[label][target_person][\"friend\"]\n",
    "                    edge_info[\"value\"] += dict_edges[label][target_person][\"value\"]\n",
    "                    edge_info[\"politics\"] += dict_edges[label][target_person][\"politics\"]\n",
    "                    edge_info[\"study\"] += dict_edges[label][target_person][\"study\"]\n",
    "                    edge_info[\"council\"] += dict_edges[label][target_person][\"council\"]\n",
    "                    edge_info[\"leftright\"] += 1  # Assuming each edge has a leftright value\n",
    "                    if \"sentiment\" in dict_edges[label][target_person]:\n",
    "                        edge_info[\"sentiment\"] += 1\n",
    "                else:\n",
    "                    # Log dropped connections\n",
    "                    dropped += 1\n",
    "                    list_dropped.append(target_person)\n",
    "\n",
    "    # Print summary of dropped connections\n",
    "    # print(\"Log dropped connections:\", unique_id, dropped)\n",
    "\n",
    "    with open(f'NA/nodes_{wave}.json', 'w') as json_file:\n",
    "        json.dump(dict_nodes, json_file, indent=4)\n",
    "\n",
    "    with open(f'NA/edges_{wave}.json', 'w') as json_file:\n",
    "        json.dump(dict_edges, json_file, indent=4)\n",
    "\n",
    "    # Print edge information summary\n",
    "    print(f\"Edge information for wave {wave}:\")\n",
    "    for key, value in edge_info.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    return dict_nodes, dict_edges, list_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge information for wave W2:\n",
      "friend: 424\n",
      "value: 273\n",
      "politics: 222\n",
      "study: 218\n",
      "council: 359\n",
      "leftright: 815\n",
      "sentiment: 815\n",
      "Edge information for wave W1:\n",
      "friend: 374\n",
      "value: 269\n",
      "politics: 252\n",
      "study: 280\n",
      "council: 359\n",
      "leftright: 750\n",
      "sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "dict_nodes_w2, dict_edges_w2, list_dropped_w2 = data_extraction(df_w2, \"participant.label\", \"W2\")\n",
    "dict_nodes_w1, dict_edges_w1, list_dropped_w1 = data_extraction(df_w1, 'network_app.1.player.participantcode', \"W1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675 87\n"
     ]
    }
   ],
   "source": [
    "sum=0\n",
    "for key,value in dict_edges_w1.items():\n",
    "    #print(key)\n",
    "    sum += len(value)\n",
    "print(sum,len(list_dropped_w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718 55\n"
     ]
    }
   ],
   "source": [
    "sum=0\n",
    "for key,value in dict_edges_w2.items():\n",
    "    #print(key)\n",
    "    sum += len(value)\n",
    "print(sum,len(list_dropped_w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w1: 675+87=762\n",
    "\n",
    "w2: 718+55=773\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_venv_3.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
